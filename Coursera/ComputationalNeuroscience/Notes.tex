%%% LaTeX Template: Article/Thesis/etc. with colored headings and special fonts
%%%
%%% Source: http://www.howtotex.com/
%%% Feel free to distribute this template, but please keep to referal to http://www.howtotex.com/ here.
%%% February 2011

%%%%% Preamble
\documentclass[10pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}

\usepackage[utf8]{inputenc}							% Input encoding
\usepackage{amsmath}									% Math

\usepackage{xcolor}
\definecolor{bl}{rgb}{0.0,0.2,0.6} 

\definecolor{background-image}{rgb}{1.0,0.9,0.7} 

% Translating the section, part, ... names into French
\usepackage[french]{babel}

% Setup the margins
\usepackage{geometry}
\newgeometry{margin=2cm}

% Customize the hyperlinks
\usepackage{hyperref}
\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={My title},    % title
    pdfauthor={Author},     % author
    pdfsubject={Subject},   % subject of the document
    pdfcreator={Creator},   % creator of the document
    pdfproducer={Producer}, % producer of the document
    pdfkeywords={keyword1} {key2} {key3}, % list of keywords
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=bl,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=bl           % color of external links
}

% For getting the caption on the side of an image
\usepackage{calc}
\usepackage{graphicx}
\usepackage{floatrow}
\floatsetup{style=ruled,footposition=caption}

\newcommand{\myfig}[2]{
\begin{figure}[htbp]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,top},capbesidewidth=4cm}}]{figure}[\FBwidth]
         {\caption{#2}\label{fig:#1}}
         {\includegraphics[width=5cm]{#1}}
\end{figure}
}


%\usepackage{sidecap}

\usepackage{sectsty}
\usepackage[compact]{titlesec} 
\allsectionsfont{\color{bl}\scshape\selectfont}

%%%%% Definitions
% Define a new command that prints the title only
\makeatletter							% Begin definition
\def\printtitle{%						% Define command: \printtitle
    {\color{bl} \centering \huge \sc \textbf{\@title}\par}}		% Typesetting
\makeatother							% End definition

\title{Computational Neuroscience \\ 
		\large \vspace*{-10pt} Notes on A. Fairhall / R.P.N. Rao lectures\vspace*{10pt}}

% Define a new command that prints the author(s) only
\makeatletter							% Begin definition
\def\printauthor{%					% Define command: \printauthor
    {\centering \small \@author}}				% Typesetting
\makeatother							% End definition

\author{%
	Jérémy Fix \\
	Jeremy.Fix@Supelec.fr \\
	\vspace{20pt}
	}

% Custom headers and footers
\usepackage{fancyhdr}
	\pagestyle{fancy}					% Enabling the custom headers/footers
\usepackage{lastpage}	
	% Header (empty)
	\lhead{}
	\chead{}
	\rhead{}
	% Footer (you may change this to your own needs)
	\lfoot{\footnotesize }
	\cfoot{}
	\rfoot{\footnotesize page \thepage\ / \pageref{LastPage}}	% "Page 1 of 2"
	\renewcommand{\headrulewidth}{0.0pt}
	\renewcommand{\footrulewidth}{0.4pt}

% Change the abstract environment
\usepackage[runin]{abstract}			% runin option for a run-in title
\setlength\absleftindent{30pt}		% left margin
\setlength\absrightindent{30pt}		% right margin
\abslabeldelim{\quad}						% 
\setlength{\abstitleskip}{-10pt}
\renewcommand{\abstractname}{}
\renewcommand{\abstracttextfont}{\color{bl} \small \slshape}	% slanted text


% Pour gérer les lettrines
\usepackage{lettrine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Start of the document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%% Top of the page: Author, Title and Abstact
\printtitle 

\printauthor

\begin{abstract}
Notes on the Computational Neuroscience lecture of A. Fairhall / R.P.N. Rao
\end{abstract}

\tableofcontents

\pagebreak

\section{Introduction \& Basic Neurobiology}

\section{What do Neurons Encode ? Neural Encoding Models}

\section{Extracting information from Neurons: Neural decoding}

Neural decoding refers to infer the stimulus that causes the neural
responses we observe.

Given a feature of a stimulus to identify, we can represent the
probability that the stimulus belongs to one category along this
feature axis. An illustration of the probability of having a stimulus
given some evidence is given by the experiment of Britten et
al. 1992. In this experiment, Monkey had to decide in which direction
random dot patterns were moving. The difficulty of the task varies and
is defined by the coherence of the pattern. If the pattern is moving
left ward, the monkey receives a reward if it makes a leftward saccade
and conversely with rightward motion. If one looks at the distribution
of the firing rates in two cases, when the monkey made a saccade
toward the preferred direction and when the monkey made a saccade
toward the null direction .....


We introduce $P(r / +)$ the probability of having response $r$ given a
stimulus is moving in the preferred direction of the neuron and $P(r /
-)$ the probability of having response $r$ given a stimulus is moving
in the null direction. Decision takes place when the response is
larger than a threshold $z$ to be defined later. We call \emph{false
  alarms} $P( r \geq z | -)$ i.e. the probability that the neurons
fire strongly while the stimulus was moving in the null direction. We
call \emph{good calls} $P(r \geq z | +)$ i.e. the probability that the
neuron fires strongly while the stimulus is moving in the preferred
direction (a good guess). We set the threshold $z$ in order to
maximize the probability to be correct, i.e. $P(+) P(r \geq z | +) +
P(-) ( 1 - P( r \geq z | -))$ i.e. the probability to fire above
threshold when the stimulus is moving in the preferred direction plus
the probability to fire below threshold when the stimulus is moving in
the null direction. The right choice of the threshold is such that
$P(r = z | -) = P(r = z | +)$. The conditional probabilities $P(r /
-)$, $P(r / +)$ are called the likelihood. \\

The likelihood ratio is the most efficient statistic, in that it has
the most power for a given size (Neyman-Pearson lemna). The likelihood
ratio being defined as $p(r/+)/p(+/-)$ and $z$ defined such that
$p(r/+)/p(+/-) > 1$.\\

If we decode the neuronal response and compare it to the behavioral
response, they match. Therefore, if there is a so close correspondence
between neuronal response (at a single cell level?) and behavior, why
do we need so many neurons ? (to build the response????)\\

We can consider the likelihood ratio over time. Suppose we have two
hypothesis for a stimulus: it might be a tiger or the wind in a bush
(breeze). Now take the log of the likelihood ratio : $l(s) = p(s /
tiger) / p(s / breeze)$. Given a sample $s$, we can compute the
likelihood ratio or its log. If it sounds like the breeze, this makes
the likelihood ratio smaller than $1$ and its log negative. As we
accumulate samples through time, our guess will fluctuate toward the
breeze or the tiger until we reach a threshold indicating we are
absolutely confident in our guess. Such a process of accumulated
evidence was demonstrated by Kiani(2006). It is similar to
Britter(1992) experiment but the monkey now had the time it wants to
respond (there is no pressure to answer quickly). Neurons in LIP were
recorded during the experiment. Depending on the coherence, we observe
different ramps with which the responses are building up (faster when
the coherence is larger, slower when the coherence is smaller). When
aligned with saccade onset, the responses peak at the small value
whatever the coherence.\\

For the tiger/breeze example, we must consider the priors $p(+), p(-)$
which scale the conditionals for taking our decision. An example of
including the priors is given by the recordings of Rieke of
photoreceptors. The response of photoreceptors is very noisy and, when
it receives a photon, is increasing suddenly. One might be interested
in the probability of having light given the signal. The priors allow
to adjust what is considered as a normal situation for the signal and
to decrease false alarms and increase good calls.\\

But priors are not the whole story. We might also include cost. The
probability of deciding an action might also depend on how costly it
is.\footnote{J'ai un peu le sentiment qu'on mélange beaucoup de chose
  : le fait de reconnaitre qu'une action devrait être engagée et le
  coût des actions engageables...}. We incorporate the cost by
considering penalties associated with wrong choices. Two losses are
therefore considered $\mbox{Loss}_- = \mbox{L}_- P(+/r)$ i.e. the cost
of choosing minus while it was a plus and $\mbox{Loss}_+ = \mbox{L}_+
P(-/r)$ i.e. the cost of choosing plus while it was minus. We now act
when we can earn more that what we can loose.  This leads to a new
criterion for a likelihood ratio test.\\

Usually, many neurons participate to the coding of an information. We
now turn to decoding of from population activities. We start with an
example of the cricket [Jacobs, 2008]. The cricket has hairs on the
abdomen that are sensitive to the wind velocity. The cricket cercal
cells have cosine receptive fields centered on a preferred direction
denoted $s_a$. The firing rate of these cells follow [Theunissen,1991]~:
\begin{equation}
\frac{f(s)}{r_{max}}_a = [cos(s - s_a]_+
\end{equation}
The response of the neuron is proportional to the projection of the
wind velocity vector on the preferred direction of the neuron $c_a$. We can
take the individual responses of the neurons and compute the
population vector as~:
\begin{equation}
\vec{v}_p = \sum_{a=1}^{4} (\frac{r}{r_{max}}) \vec{c_a}
\end{equation}
We here have $4$ basis vectors $\vec{c_a}$ equally arranged around the circle of
unit radius. This might seems to be overcomplete but as we take the
positive part of the cosine, each neuron is only responsive for an
half-plane. We can compute the population vector for other populations
of neurons like in M1. In the motor cortex, we also observe individual
responses with cosine tuning~:
\begin{equation}
\frac{<r> - r_0}{r_{max}} = \frac{f(s) - r_0}{r_{max}} = cos(s - s_a)
\end{equation}

The population vector reads~:
\begin{equation}
\vec{v}_p = \sum_a \frac{r - r_0}{r_{max}} \vec{c_a}
\end{equation}
For sufficiently large number of neurons in the population~:
\begin{equation}
<\vec{v}_p> = \sum_a (\vec{v} \vec{c_a}) \vec{c_a}
\end{equation}
which is parallel to the direction of the arm movement.\\

The population vector is one technique of decoding but that appears to
be quite specific to cosine tuning. We try to derive a more general
formulation of population decoding. This formulation is based on the
Bayes law~:
\begin{equation}
p[s / r] = \frac{p(r/s)p(s)}{p(r)} = \frac{p(r/s)p(s)}{\int_s ds p(r/s)p(s)}
\end{equation}
In the Bayesian framework, $p(r/s)$ is the likelihood function which
is the probability of having response $r$ given the stimulus $s$ is
presented; $p(s)$ is the a priori distribution over the stimuli and
$p(r)$ is the marginal distribution which normalizes the
probabilities. $p(s/r)$ is then the a posteriori distribution which is
the probability of having presented stimulus $s$ if we observe the
response $r$. Indeed, this is what we seek to compute when decoding
responses. We focus on two of the distributions, the likelihood and
the a posteriori distribution and derives two general decoding
techniques. \textbf{Maximum likelihood} (ML) seeks $s*$ that maximizes the
likelihood $p(r/s)$. \textbf{Maximum a posteriori} (MAP) seeks $s*$ which
maximizes the a posteriori distribution $p(s/r)$. These two methods
give different estimates as the MAP is biased by the prior.\\


\textbf{Question} : quels sont les avantages de l'une ou l'autre des
max likelihood ou maximum a posteriori. Genre le plus précis mais peut
être nécessitant plus d'info ?\\

Let us consider an example of a population of neurons coding some
stimulus $s$. Assume that the tuning curves are gaussians centered on
some preferred values of the stimulus $s_i, i \in [1..N]$. We assume
\textbf{independence} in the responses and \textbf{poisson
  firing}. The response of the neurons read~:
\begin{equation}
f_a(s) = r_{max} \exp(-\frac{s - s_a}{2 \sigma_a^2})
\end{equation}
We also assume a good coverage in the sense that there are enough
neurons to encode any stimulus value and that they are equally
spaced~:$\sum_a f_a(s)$ is constant $\forall s$. If we record the
response of a neuron we usually have a lot of variability. However, if
we average the responses to several presentations of the stimuli, we
may have the gaussian tuning curve. With a Poisson neuron, the
probability of having $k$ spikes during a time interval $\Delta t$ is
given by $e^{-r\Delta t} \frac{(r \Delta t)^k}{k!}$. The average
firing rate being $r$, its standard deviation $\sqrt{r}$.\\

\textbf{TODO} : se revoir les distributions de Poisson.\\

Suppose a neuron has response $r_a$ given a stimulus $s$. We want to
compute the probability of having this response given our assumption
of Poisson neurons. \\
\begin{eqnarray}
P_T(r_a/s) = \frac{(f_a(s) T)^{r_a T}}{(r_aT)!} \exp(-f_a(s) T)
\end{eqnarray}
We denote $\vec{r}$ the vector of all the responses $r_1, r_2,
... r_N$. As we assumed independence, the probability of all the
responses is just the product of the individual probabilities~:
\begin{equation}
P_T(\vec{r}/s) = \prod_{i=1}^N P_T(r_i/s) = \prod_{i=1}^N  \frac{(f_i(s)
  T)^{r_i T}}{(r_i T)!} \exp(-f_i(s) T)
\end{equation}
We now have the expression of the likelihood. Applying the ML
estimator, we seek the stimulus $s*$ which maximizes it which is
equivalent to maximizing the log likelihood. The log likelihood has a
much easier expression and reads~:
\begin{eqnarray}
\log(P_T(\vec{r}/s)) &=& \sum_i [r_i T \log(f_a(s) T) - f_a(s)T -
\log((r_a T)!)]
\end{eqnarray}
To find the maximum, we derive the log likelihood.
\begin{eqnarray}
\frac{d\log(P_T(\vec{r}/s))}{ds} = \sum_i \frac{d f_i(s)}{ds}\frac{r_i
T}{f_i(s)}
\end{eqnarray}
The term $\log((r_a T)!)$ gets dropped because of its independence on
$s$. The other term $\sum_if_a(s)T$ also gets dropped as we assumed
good coverage, i.e. the sum of the responses is constant whatever the
stimulus. Setting the derivative to zero, one gets~:
\begin{eqnarray}
\frac{d\log(P_T(\vec{r}/s))}{ds}(s*) = 0 \rightarrow \sum_i \frac{d f_i(s*)}{ds}\frac{r_i
T}{f_i(s*)} = 0
\end{eqnarray}
We supposed a gaussian tuning curve, i.e. $f(s) = A \exp(\frac{-(s -
  s_a)^2}{2 \sigma_a^2})$. Therefore $\frac{f'(s)}{f(s)} =
-\frac{s-s_a}{\sigma_a}^2$, which simplifies our expression for
finding $s*$~:
\begin{equation}
\sum_i -r_i T \frac{s*-s_i}{\sigma_i^2} = 0 \rightarrow s* =
\frac{\sum_i \frac{r_i s_i}{\sigma_i^2}}{\sum_i \frac{r_i}{\sigma_i^2}}
\end{equation}
which is a kind of population vector with the weights being normalized
by the variance of the responses. Indeed, if all the variances are the
same, the maximum likelihood decoding is nothing more than the
population vector.\\

To compute the MAP estimator, we need to compute the a posteriori
distribution.\\
\begin{equation}
\log(p(s/r)) = \log(p(r/s)) + \log(p(s)) - \log(p(r))
\end{equation}
If we inject our expressions of the log likelihood, putting in $...$
the terms that will get dropped when computing the derivative with
respect to $s$, we get~:
\begin{equation}
\log(p(s/r)) = T \sum_i r_i \log(f_i(s)) + \log(p(s)) + ...
\end{equation}
Setting the derivative to $0$, we get an expression somehow similar to
the ML estimator, except that we now have a term depending on the
prior~:
\begin{equation}
\sum_i r_i \frac{f_i'(s*)}{f_i(s*)} + \frac{p'(s)}{p(s)} = 0
\end{equation}

Assuming a Gaussian prior of mean $s_{\mbox{prior}}$ and standard
deviation $\sigma_{\mbox{prior}}$, we finally get the MAP estimate~:
\begin{equation}
s* = \frac{T \sum_i \frac{r_i s_i}{\sigma_i^2} +
  \frac{s_{\mbox{prior}}}{\sigma_{\mbox{prior}}^2}}{T\sum_i
  \frac{r_i}{\sigma_i^2} + \frac{1}{\sigma_{\mbox{prior}}^2}}
\end{equation}
The priori therefore shifts the estimate. If we consider a priori
close to a dirac, very peaky around the mean, the priori will strongly
bias the estimate whatever the responses. If, on the contrary, the
prior is flat $\sigma_{\mbox{prior}} \rightarrow \infty$, the
influence of the prior completely disappears and we get back to the ML
estimator.\\

These estimators have limitations. Especially, they do need
independence in the responses while correlation of the responses is
critical as we will see later on.\\

\subsection{Reading minds: stimulus reconstruction}

In the previous parts, we were interested in decoding a stimulus
$\star{s}$ that is held fixed during recording. We now turn to
decoding a time varying stimulus $\star{s}(t)$.\\

We look for an estimator $s_{\mbox{bayes}}$ that gives the best
approximation of stimulus $s$ when we observe $r$. To do so, we
introduce a cost function $L(s, s_{\mbox{bayes}})$ and make use of it
to introduce a criteria to be minimized~:
\begin{equation}
J = \int ds L(s, s_{\mbox{bayes}}) p[s/r]
\end{equation}
One example of such a cost function is the least square cost $L(s,
s_{\mbox{bayes}}) = (s - s_{\mbox{bates}})^2$. To minimize the cost
$J$, we compute its derivative relative to $s_{\mbox{bayes}}$. Using
the least square, this reads~:
\begin{eqnarray}
\frac{dJ}{d s_{\mbox{bayes}}} &=& 2 \int ds(s - s_{\mbox{bayes}})
p[s/r]\\
&=& 0 \rightarrow s_{\mbox{bayes}} = \int ds p[s/r] s
\end{eqnarray}
since $\int ds p[s/r] = 1$. If the response is a single spike, we
recognize the spike triggered average.\\

There have been some experiments with LGN recordings of the cat to
decode what the cat actually sees (Dan Yang). There are also some
experiments in 2011 by Nishimoto on reconstruction from fMRI.\\


\subsection{Fred Rieke, guest lecture on the retina}

Imagine a lot of rods, few of them absorbing some photons. We
therefore get an array of very noisy detectors. Averaging is not a
good strategy, we should rather select and weight the contribution of
each detector. We seek to set up a threshold above which rods will be
kept, and supposed to be excited by photons while below the threshold
the rods will be considered as generating noise. We can therefore
introduce two probability distributions which is, for rods generating
noise and for rods generating some signal, the probability
distribution over the amplitude of the signal. This selection of good
vs. noisy rods should occur very early in the retina, at bipolar cell
layer otherwise we might already average out the signal. Indeed,
recordings from rod bipolar cells support the idea of a thresholding
nonlinearity. If we plot this non-linearity function of the amplitude
of the signal, it is very sharp and the threshold and somehow higher
than what we would expect from a maximum likelihood. Indeed, one
should also introduce prior probabilities to generate noise (99.9
percent) or signal. How single photon responses are reliably
transmitted (by filtering out most of the noisy responses) is one of
the challenges but there are also other aspects before and after this
transmission such as the transduction of single photon by rods, as
well as how the signal is then transmitted through the optic nerve to
the brain.



\section{Information Theory and Neural Coding}



\section{Bidirectional BCI (E. Fetz)}

Bidirectional BCI is a new paradigm introduced by E. Fetz where some
neurons are recorded (or some EEG, ECOG\footnote{electrocorticogram}, LFP, ..), processed in real
time by an electronic device and then sent back to the brain. The
electronic device, the so-called neurochip, is presented in
Mavoori(2005) for primates. There two types of applications of
bidirectional BCI~:
\begin{itemize}
\item create artificial recurrent connections that the brain will
  incorporate in its processing. It might compensate loss of
  connections,
\item create synaptic plasticity.
\end{itemize}

\subsection{Creating artificial recurrent connections}

Monkeys could learn to use these interfaces to control electrical
stimulation of paralyzed muscles [Moritz, Perlmutter(2008)]. The
monkey can learn the task in 10 to 20 minutes. The muscles can be
driven by two different cells (flexion and extension depends on the
activity of the respective cells being higher than a threshold). They
can also be controlled by a single cell where what is monitored is
whether the activity is higher than a flexor threshold or lower than a
extensor threshold. The important point is also is that it is not
necessary to monitor cells normally involved in controlling the
muscles. The extension from one to multiple muscles is not
straightforward as the contraction/release of different muscles is
usually coordinated. Another site of stimulation is the spinal
cord. Spinal sites typically evoke coordinated movements. 

\subsection{Plasticity of corticospinal connections}

There are some neurons that have monosynaptic connections to the
muscles [Cheney, Fetz (1980)]. These can be identified by
computing spike-triggered averages of muscle contraction. Then, you
can check which muscles are facilitated when the neuron is triggering
spikes (so called post spike facilitation). These neurons get
recorded, feeding the neurochip which stimulates the target of the
recorded neurons in the spinal cord. The stimulation is set so that
they are in synchrony with the signals received directly from the
motor neurons [Nishimura et aL.]. After a full day training, the post
spike effect is increased. The effect is greater for a specific delay
(around 20 ms) between a recorded spike and the intraspinal
stimulation. Interestingly, if the stimulation arrives too early, the
post-synaptic spinal neuron gets excited before receiving the afferent
stimulation from the moto-neuron, and this indeed leads to a decrease
of the post spike facilitation.


Leuthardt et al 2007 : boucle BCI standard avec en entrée EEG, ECOG ou
LFP. Ils présentent aussi le bidirectional ou stimule des muscles,
spinal cord, ou le cerveau de manière générale.

\end{document}
